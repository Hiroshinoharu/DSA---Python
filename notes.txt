Data Structures and Algorithms

Program is set of instructions which performs operation on Data

How memory is organized.

Data Structures is a way of organizing Data

Algorithm is a set of steps to solve a problem

Searching, Sorting, and other operations are performed using algorithms.

Data Structures and Algorithms are fundamental concepts in computer science that help in organizing and processing data efficiently.

Sample Data Structures:

1. Arrays
2. Linked Lists
3. Stacks
4. Queues
5. Trees
6. Graphs
7. Hash Tables
8. Tries
9. Heaps
10. Sets

Sample Algorithms:

1. Sorting Algorithms (e.g., Quick Sort, Merge Sort, Bubble Sort)
2. Searching Algorithms (e.g., Binary Search, Linear Search)
3. Graph Algorithms (e.g., Dijkstra's Algorithm, Depth-First Search, Breadth-First Search)
4. Dynamic Programming Algorithms (e.g., Fibonacci Sequence, Knapsack Problem)
5. Greedy Algorithms (e.g., Prim's Algorithm, Kruskal's Algorithm)
6. Backtracking Algorithms (e.g., N-Queens Problem, Sudoku Solver)
7. Divide and Conquer Algorithms (e.g., Merge Sort, Quick Sort)
8. String Algorithms (e.g., KMP Algorithm, Rabin-Karp Algorithm)
9. Mathematical Algorithms (e.g., Euclidean Algorithm for GCD, Sieve of Eratosthenes for prime numbers)
10. Bit Manipulation Algorithms (e.g., Counting Set Bits, Finding Unique Elements)
11. Tree Traversal Algorithms (e.g., Inorder, Preorder, Postorder Traversal)

Concepts:

Big O Notation: A mathematical notation used to describe the upper bound of an algorithm's time complexity, providing a high-level understanding of its efficiency.

Time Complexity: A measure of the amount of time an algorithm takes to complete as a function of the length of the input.

Space Complexity: A measure of the amount of memory an algorithm uses as a function of the length of the input.

Recursion: A programming technique where a function calls itself to solve smaller instances of the same problem.

Memory Management: The process of allocating and deallocating memory in a program, which is crucial for efficient data structure implementation.

How to follow this course:

Weekly schedule is important for consistent learning.

Set aside specific times each week to focus on studying data structures and algorithms.

Practice coding problems regularly to reinforce your understanding.

Don't rush through the topics; take your time to understand each concept thoroughly.

Big O Notation
====================

Big O Notation is a mathematical notation used to describe the upper bound of an algorithm's time complexity. 
It provides a high-level understanding of how the algorithm's performance scales with the size of the input.

Good Code: 
- Run fast
- Requires less memory
- Readable, maintainable, scalable (Time dependent and Space dependent)

Time Complexity
====================
Time Complexity is a measure of the amount of time an algorithm takes to complete as a function of the length of the input. 
It helps in analyzing how the execution time of an algorithm grows with the size of the input data.

Space Complexity: A measure of the amount of memory an algorithm uses as a function of the length of the input. 
It is important to consider both time and space complexity when evaluating algorithms.
Space Complexity is crucial for understanding how much memory an algorithm will consume, which can impact performance and scalability.

Space Complexity is a measure of the amount of memory an algorithm uses as a function of the length of the input.

Recursion

Recursion is a programming technique where a function calls itself to solve smaller instances of the same problem.
It is often used to solve problems that can be broken down into smaller, similar subproblems


Complexity Analysis:

1. Constant Time: O(1)
   - The algorithm takes the same amount of time regardless of the input size.
   - Example: Accessing an element in an array by index.

2. Logarithmic Time: O(log n)
   - The algorithm takes time proportional to the logarithm of the input size.
   - Example: Binary search in a sorted array.

3. Linear Time: O(n)
    - The algorithm takes time proportional to the input size.
    - Example: Iterating through all elements in an array.

4. Linearithmic Time: O(n log n)
    - The algorithm takes time proportional to n times the logarithm of n.
    - Example: Efficient sorting algorithms like Merge Sort and Quick Sort.

5. Quadratic Time: O(n^2)
    - The algorithm takes time proportional to the square of the input size.
    - Example: Nested loops iterating through an array.

5. Cubic Time: O(n^3)
   - The algorithm takes time proportional to the cube of the input size.
   - Example: Triple nested loops iterating through a 3D matrix.

6. Exponential Time: O(2^n)
    - The algorithm takes time proportional to 2 raised to the power of the input size.
    - Example: Recursive algorithms that solve problems by exploring all possible combinations, such as the Fibonacci sequence.

7. Factorial Time: O(n!)
    - The algorithm takes time proportional to the factorial of the input size.
    - Example: Generating all permutations of a set of elements.

8. Polynomial Time: O(n^k)
    - The algorithm takes time proportional to a polynomial function of the input size.
    - Example: Algorithms that involve nested loops where the number of iterations is a polynomial function of the input size.

Memory 

1 memory slot = 8 bits = 1 byte = 2^8 = 256 values

Endianess : The order in which bytes are stored in memory. There are two types of endianness:
- Big Endian: Most significant byte first
- Little Endian: Least significant byte first


Logarithm is the inverse operation to exponentiation, meaning it answers the question: "To what exponent must a base be raised to produce a given number?"
For example, log base 2 of 8 is 3, because 2 raised to the power of 3 equals 8 (2^3 = 8). 

Logarithm is usually base 2, but can also be base 10 or base e (natural logarithm).

Data structure is a way of organizing data in a computer so that it can be used efficiently.

Different data structures are suited for different kinds of applications, and some are highly specialized to specific tasks.

Choosing the right data structure can greatly impact the efficiency and performance of an algorithm.

Data structures can be classified into two main categories:

1. Primitive Data Structures: These are the basic data types provided by programming languages, such as integers, floats, characters, and booleans.

2. Non-Primitive Data Structures: These are more complex data structures that are built using primitive data types. They include arrays, linked lists, stacks, queues, trees, graphs, and hash tables.

Data structures can also be categorized based on their organization and access patterns:

1. Linear Data Structures: These data structures store elements in a sequential manner, where each element is connected to its previous and next element. Examples include arrays, linked lists, stacks, and queues.

2. Non-Linear Data Structures: These data structures store elements in a hierarchical or interconnected manner, where elements can have multiple connections. Examples include trees and graphs.

3. Static Data Structures: These data structures have a fixed size and cannot grow or shrink during runtime. Examples include arrays.

4. Dynamic Data Structures: These data structures can grow or shrink in size during runtime, allowing for more flexibility. Examples include linked lists, stacks, queues, trees, and hash tables.

Data structures can also be classified based on their access patterns:
1. Sequential Access: Elements are accessed in a linear order, one after the other. Examples include arrays and linked lists.
2. Random Access: Elements can be accessed directly using an index or key. Examples include arrays and hash tables.
3. Hierarchical Access: Elements are accessed based on their relationships in a tree or graph structure. Examples include trees and graphs.
Data structures are essential for efficient data management and manipulation in computer programs. They provide a way to store, organize, and retrieve data effectively, enabling algorithms to perform operations on that data efficiently.
Data structures can also be classified based on their memory allocation:

1. Contiguous Memory Allocation: Data structures that store elements in contiguous memory locations, such as arrays.
2. Non-Contiguous Memory Allocation: Data structures that store elements in non-contiguous memory locations, such as linked lists and trees.
Data structures can also be classified based on their operations:
1. Mutable Data Structures: These data structures allow modification of their elements after creation. Examples include lists, sets, and dictionaries in Python.
2. Immutable Data Structures: These data structures do not allow modification of their elements after creation. Examples include tuples and strings in Python.
3. Persistent Data Structures: These data structures maintain previous versions of themselves when modified, allowing access to both the old and new versions. Examples include functional programming data structures like immutable lists and trees.

Data structures can also be classified based on their usage patterns:
1. Single-Threaded Data Structures: These data structures are designed for use in single-threaded

applications, where only one thread accesses the data structure at a time. Examples include standard arrays and linked lists.

2. Multi-Threaded Data Structures: These data structures are designed for concurrent access by multiple threads, providing mechanisms to handle synchronization and prevent data races. Examples include concurrent queues and thread-safe collections.

Arrays

items are stored in contiguous memory locations (One after another)

2 types of arrays:

Static Arrays: Fixed size, allocated at compile time. Cannot grow or shrink during runtime.

Complexity of Static Arrays: O(1) for access, O(n) for insertion and deletion at arbitrary positions due to shifting elements.

Dynamic Arrays: Can grow or shrink during runtime, allocated at runtime. Examples include Python lists and Java ArrayLists.

Complexity of Dynamic Arrays: Best case: O(1) for insertion and deletion at the end. Worst case: O(n) for insertion and deletion at arbitrary positions due to shifting elements.

Arrays Operation Complexity:

1. Access: O(1) - Direct access to elements using an index.

2. Update: O(1) - Direct access to elements using an index.

3. Traversal: O(n) - Iterating through all elements in the array.

4. Search: O(n) - Linear search through the array. O(log n) for binary search in a sorted array.

5. Copy : O(n) - Copying all elements to a new array.

6. Insert: O(n) - Inserting an element requires shifting elements to maintain order.

7. Delete: O(n) - Deleting an element requires shifting elements to maintain order.

Linked Lists

Linked lists are a type of data structure where each element (node) contains a value and a reference (or pointer) to the next node in the sequence. This allows for dynamic memory allocation and efficient insertion and deletion operations.

Linked lists can be classified into several types:
1. Singly Linked Lists: Each node only contains a reference to the next node.
2. Doubly Linked Lists: Each node contains references to both the previous and next nodes.
3. Circular Linked Lists: The last node points back to the first node, forming a circular structure.

In memory canvas , linked lists are stored as a sequence of nodes, each containing a value and a reference to the next node. The nodes are not stored in contiguous memory locations, allowing for dynamic resizing.

Complexity of Singly Linked Lists:
1. Access: O(n) - Requires traversal from the head node to reach a specific node.
2. Update: O(n) - Requires traversal to find the node to update.
3. Traversal: O(n) - Iterating through all nodes in the linked list.
4. Delete: O(n) - Requires traversal to find the node to delete, then updating the references.
5. Search: O(n) - Linear search through the linked list.
6. Insert: O(1) - Inserting at the head or tail is efficient, but O(n) if inserting at an arbitrary position due to traversal.

Doubly Linked Lists:

Doubly linked lists are a type of linked list where each node contains references to both the previous and next nodes. This allows for more efficient traversal in both directions and simplifies certain operations like deletion.

Complexity of Doubly Linked Lists:
1. Access: O(n) - Requires traversal from the head node to reach a specific node.
2. Update: O(n) - Requires traversal to find the node to update.
3. Traversal: O(n) - Iterating through all nodes in the doubly linked list.
4. Delete: O(n) - Requires traversal to find the node to delete, then updating both previous and next references.
5. Search: O(n) - Linear search through the doubly linked list.

Cirtcular Linked Lists:

Circular linked lists are a type of linked list where the last node points back to the first node, forming a circular structure. This allows for continuous traversal without reaching a null reference.

Complexity of Circular Linked Lists:
1. Access: O(n) - Requires traversal from the head node to reach a specific node.
2. Update: O(n) - Requires traversal to find the node to update.
3. Traversal: O(n) - Iterating through all nodes in the circular linked list.
4. Delete: O(n) - Requires traversal to find the node to delete, then updating the references.
5. Search: O(n) - Linear search through the circular linked list.

Stack:

A stack is a linear data structure that follows the Last In First Out (LIFO) principle. Elements can be added (pushed) and removed (popped) from the top of the stack only.

Think of a stack as a collection of items where the last item added is the first one to be removed, similar to a stack of plates where you can only take the top plate off.

You can visualize a stack as a vertical collection of items, where you can only interact with the top item. The stack has two main operations: push to add an item and pop to remove the top item. Additionally, you can peek at the top item without removing it, and you can also keep track of the minimum and maximum items in the stack for quick access.

A stack can be implemented as an array or a linked list. The key operations are:

- Push: Add an element to the top of the stack.
- Pop: Remove the top element from the stack.
- Peek(Top): Return the top element without removing it.
- Min: Return the minimum element in the stack.
- Max: Return the maximum element in the stack.

Min Stack: The min stack is a stack that keeps track of the minimum element at each level. It's used to efficiently retrieve the minimum element in constant time. When you push an element onto the min stack, you also check if it's smaller than the current minimum and update the minimum accordingly. When you pop an element, you also check if it was the minimum and update the minimum if necessary.

Max Stack: Similar to the min stack, the max stack keeps track of the maximum element at each level. It allows you to retrieve the maximum element in constant time. When pushing an element, you check if it's larger than the current maximum and update it accordingly. When popping, you check if it was the maximum and update if necessary.

Complexity:
- Push: O(1) - adding an element to the top of the stack is a constant time operation.
- Pop: O(1) - removing the top element is also a constant time operation.
- Peek: O(1) - checking the top element without removing it is constant time.
- Min: O(1) - retrieving the minimum element is constant time.
- Max: O(1) - retrieving the maximum element is constant time.
- Delete: O(1) - deleting the top element is constant time.

Queue:

A queue is a linear data structure that follows the First In First Out (FIFO) principle. Elements can be added (enqueued) at the back and removed (dequeued) from the front of the queue only.

Think of a queue as a line of people waiting for service, where the first person in line is the first one to be served.

You can visualize a queue as a horizontal collection of items, where you can add items to the back and remove items from the front. The queue has two main operations: enqueue to add an item and dequeue to remove the front item. Additionally, you can peek at the front item without removing it.

A queue can be implemented as an array or a linked list. The key operations are:

- Enqueue: Add an element to the back of the queue.
- Dequeue: Remove the front element from the queue.
- Peek(Front/Top): Return the front element without removing it.

Complexity:
- Enqueue: O(1) - adding an element to the back of the queue is a constant time operation. However, if the queue is implemented as an array and it's full, you may need to resize it, which can take O(n) time in that case.
- Dequeue: O(1) - removing the front element is also a constant time operation unless resizing is needed. In that case, it's O(n) time.
- Peek: O(1) - checking the front element without removing it is constant time.

Hash Tables:

Built-in hash tables are data structures that store key-value pairs, allowing for efficient data retrieval based on keys.

They use a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.

Search Complexity:
- Average Case: O(1) - searching for an element in a hash table is typically a constant time operation due to the use of hash functions.
- Worst Case: O(n) - in cases of hash collisions, where multiple elements are mapped to the same index, the search time can degrade to linear time.
- Best Case: O(1) - when there are no collisions, the search time is constant.

Variable {Key: value}

Hash tables are often implemented using arrays, where each index corresponds to a bucket that can hold multiple key-value pairs in case of collisions. The hash function is used to determine the index for each key.

When a key-value pair is added, the hash function computes the index, and the pair is stored in the corresponding bucket. If a collision occurs (i.e., another key hashes to the same index), various strategies can be used to handle it, such as chaining (storing multiple pairs in a linked list at that index) or open addressing (finding another empty slot).

Hash Tables collision resolution strategies:
- Chaining: Each index in the hash table points to a linked list of entries that hash to the same index. When a collision occurs, the new entry is added to the linked list at that index.
- Open Addressing: When a collision occurs, the algorithm searches for the next available slot in the array. This can be done using various probing techniques, such as linear probing, quadratic probing, or double hashing.
- Linear Probing: If a collision occurs, the algorithm checks the next index in the array until it finds an empty slot.
- Quadratic Probing: Similar to linear probing, but the step size increases quadratically (1, 4, 9, etc.) to reduce clustering.
- Double Hashing: Uses a second hash function to determine the step size for probing, which helps to reduce clustering and improve distribution of entries.

Hash tables are widely used in programming languages and libraries as built-in data structures, such as JavaScript objects, Python dictionaries, and Java HashMaps. They provide efficient key-value storage and retrieval, making them suitable for various applications like caching, indexing, and associative arrays.

// Example of a simple hash table implementation in JavaScript
class HashTable {
    constructor() {
        this.table = new Array(137); // Initialize an array of size 137
    }
    hash(key) {
        let total = 0;
        for (let char of key) {
            total += char.charCodeAt(0);
        }
        return total % this.table.length; // Return the index based on the hash function
    }

    set(key, value) {
        const index = this.hash(key);
        if (!this.table[index]) {
            this.table[index] = [];
        }
        // Check for existing key and update value if found
        for (let i = 0; i < this.table[index].length; i++) {
            if (this.table[index][i][0] === key) {
                this.table[index][i][1] = value;
                return;
            }
        }
        // If key not found, add new key-value pair
        this.table[index].push([key, value]);
    }

    get(key) {
        const index = this.hash(key);
        if (this.table[index]) {
            for (let i = 0; i < this.table[index].length; i++) {
                if (this.table[index][i][0] === key) {
                    return this.table[index][i][1]; // Return the value if key is found
                }
            }
        }
        return undefined; // Return undefined if key is not found
    }

    remove(key) {
        const index = this.hash(key);
        if (this.table[index]) {
            for (let i = 0; i < this.table[index].length; i++) {
                if (this.table[index][i][0] === key) {
                    this.table[index].splice(i, 1); // Remove the key-value pair
                    return true; // Return true if removed successfully
                }
            }
        }
        return false; // Return false if key was not found
    }
   
    keys() {
        const keysArray = [];
        for (let bucket of this.table) {
            if (bucket) {
                for (let pair of bucket) {
                    keysArray.push(pair[0]); // Collect all keys
                }
            }
        }
        return keysArray; // Return an array of all keys
    }

    values() {
        const valuesArray = [];
        for (let bucket of this.table) {
            if (bucket) {
                for (let pair of bucket) {
                    valuesArray.push(pair[1]); // Collect all values
                }
            }
        }
        return valuesArray; // Return an array of all values
    }

    clear() {
        this.table = new Array(137); // Reset the hash table
    }

    size() {
        let count = 0;
        for (let bucket of this.table) {
            if (bucket) {
                count += bucket.length; // Count all key-value pairs
            }
        }
        return count; // Return the total number of key-value pairs
    }

    isEmpty() {
        return this.size() === 0; // Check if the hash table is empty
    }
}

// Example usage of the HashTable class
const hashTable = new HashTable();
hashTable.set("name", "Alice");
hashTable.set("age", 30);
console.log(hashTable.get("name")); // Output: Alice
console.log(hashTable.get("age")); // Output: 30
console.log(hashTable.keys()); // Output: ["name", "age"]
console.log(hashTable.values()); // Output: ["Alice", 30]
// Clear the hash table
hashTable.clear();
// Check if the hash table is empty
console.log(hashTable.isEmpty()); // Output: true

Hash table Complexity:
- Set: O(1) on average, O(n) in the worst case due to collisions.
- Get: O(1) on average, O(n) in the worst case due to collisions.
- Remove: O(1) on average, O(n) in the worst case due to collisions.
- Keys: O(n) - iterating through all keys in the hash table.
- Values: O(n) - iterating through all values in the hash table.
- Clear: O(1) - resetting the hash table.

Tree:

A tree is a data structure composed of nodes, where each node has a value and zero or more child nodes.
The top node is called the root, and nodes with no children are called leaves.
Trees are used to represent hierarchical relationships.

A tree can be visualized as an inverted tree structure, where the root is at the top and branches extend downwards.
Each node can have multiple children, but each child has only one parent, except for the root which has no parent.

Node - Node is the basic unit of a tree, containing a value and references to its children.
Root - The topmost node in a tree, which has no parent.
Parent - A node that has one or more children.
Child - A node that is directly connected to another node when moving away from the root.
Leaf - A node that has no children, representing the end of a branch in the tree.
Sibling - Nodes that share the same parent are called siblings.
edge - The connection between two nodes in a tree is called an edge.
levels - The depth of a node is the number of edges from the root to that node. The root is at level 0, its children are at level 1, and so on.
Subtree - A subtree is a tree formed by a node and all its descendants. Each node in a tree can be considered the root of a subtree.
Path - A path in a tree is a sequence of nodes and edges connecting a node with a descendant. The path from the root to a leaf is often referred to as the tree's height.

Types of Trees:
- Binary Tree: Each node has at most two children, referred to as the left and right child.
- Binary Search Tree (BST): A binary tree where the left child is less than the parent node and the right child is greater than the parent node. This property allows for efficient searching, insertion, and deletion operations.
- Balanced Tree: A tree where the height of the left and right subtrees of any node differ by at most one. Examples include AVL trees and Red-Black trees.
- N-ary Tree: A tree where each node can have up to N children. This is a generalization of binary trees.
- Trie: A specialized tree used for storing strings, where each node represents a character of a string. It is commonly used for autocomplete and spell-checking applications.

Binary Tree:

A binary tree is a tree data structure where each node has at most two children, referred to as the left child and the right child.
The root node is the topmost node in the tree, and it has no parent.
A binary tree can be visualized as a hierarchical structure where each node can have up to two branches extending downwards. The left child is typically less than the parent node, and the right child is greater than the parent node in a binary search tree (BST).

Types of Binary Trees:

- Full Binary Tree: Every node has either 0 or 2 children. All leaves are at the same level.
- Complete Binary Tree: Every node has either 0 or 2 children, and all levels are fully filled except possibly for the last level, which is filled from left to right.
- Perfect Binary Tree: All internal nodes have two children, and all leaves are at the same level. The number of nodes at each level doubles as you move down the tree.
- Balanced Binary Tree: A binary tree where the height of the left and right subtrees of any node differ by at most one. This ensures that the tree remains balanced, allowing for efficient operations.
- Unbalanced Binary Tree: A binary tree where the height of the left and right subtrees of any node can differ by more than one, leading to inefficient operations.

Binary Search Tree (BST):

A binary search tree is a binary tree where the left child is less than the parent node and the right child is greater than the parent node. 
This property allows for efficient searching, insertion, and deletion operations.

Types of Binary Search Trees:

Balanced BST: A binary search tree where the height of the left and right subtrees of any node differ by at most one. Examples include AVL trees and Red-Black trees.
Unbalanced BST: A binary search tree where the height of the left and right subtrees of any node can differ by more than one, leading to inefficient operations.

Operations on Binary Search Trees:
- Insertion: Inserting a new node into the tree while maintaining the BST property.
- Deletion: Removing a node from the tree while maintaining the BST property.
- Searching: Finding a node with a specific value in the tree.

Complexity of Binary Search Trees:
- Insertion: O(log n) on average, O(n) in the worst case (unbalanced tree).
- Deletion: O(log n) on average, O(n) in the worst case (unbalanced tree).
- Searching: O(log n) on average, O(n) in the worst case (unbalanced tree).
- Access: O(log n) on average, O(n) in the worst case (unbalanced tree).

AVL Trees:

AVL trees are a type of self-balancing binary search tree where the height of the left and right subtrees of any node differ by at most one. 
This ensures that the tree remains balanced, allowing for efficient operations.

AVL trees maintain their balance through rotations during insertion and deletion operations. 
When the balance factor (height of left subtree - height of right subtree) exceeds 1 or -1, rotations are performed to restore balance.

AVL Tree Operations:
- Insertion: O(log n) - Inserting a new node while maintaining the AVL property. If the tree becomes unbalanced, rotations are performed to restore balance.
- Deletion: O(log n) - Removing a node while maintaining the AVL property. If the tree becomes unbalanced, rotations are performed to restore balance.

AVL Tree Complexity:
- Insertion: O(log n) - Inserting a new node while maintaining the AVL property. If the tree becomes unbalanced, rotations are performed to restore balance.
- Deletion: O(log n) - Removing a node while maintaining the AVL property. If the tree becomes unbalanced, rotations are performed to restore balance.

Red-Black Trees:
Red-Black trees are a type of self-balancing binary search tree where each node is either red or black.
The tree maintains its balance through specific properties and rotations during insertion and deletion operations.

Red-Black Tree Properties:
1. Each node is either red or black.
2. The root node is always black.
3. Every leaf node (NIL) is black.
4. If a red node has children, both children must be black (no two red nodes can be adjacent).
5. Every path from a node to its descendant leaf nodes must have the same number of black nodes (black height).

Red-Black Tree Operations:
- Insertion: O(log n) - Inserting a new node while maintaining the Red-Black properties. 

If the tree becomes unbalanced, rotations and color changes are performed to restore balance.

Heaps:

A heap is a specialized tree-based data structure that satisfies the heap property.

There are two types of heaps:
1. Max Heap: In a max heap, for any given node, the value of that node is greater than or equal to the values of its children.
2. Min Heap: In a min heap, for any given node, the value of that node is less than or equal to the values of its children.

Heaps are often implemented as binary trees, but they can also be represented as arrays. The root node is the maximum (or minimum) element in the heap, and the tree is structured such that each parent node is greater than (or less than) its children.

Heap Operations:
- Insertion: O(log n) - Inserting a new element into the heap while maintaining the heap property. 
The new element is added at the end of the heap and then "bubbled up" to restore the heap property.

- Deletion: O(log n) - Removing the root element (maximum or minimum) from the heap while maintaining the heap property. 
The last element is moved to the root and then "bubbled down" to restore the heap property.

Heap Complexity:
- Insertion: O(log n) - Inserting a new element into the heap while maintaining the heap property.
- Deletion: O(log n) - Removing the root element (maximum or minimum) from the heap while maintaining the heap property. 
- Access: O(1) - Accessing the root element (maximum or minimum) is done in constant time.

Insertion steps:
1. Add the new element at the end of the heap (as the last node).
2. "Bubble up" the new element to restore the heap property by comparing it with its parent and swapping it with the parent until the heap property is satisfied.

Deletion steps:
1. Remove the root element (maximum or minimum).
2. Move the last element to the root.
3. "Bubble down" the new root element to restore the heap property by comparing it with its children and swapping it with the larger (or smaller) child until the heap property is satisfied.

Deletion steps:

1. Remove the root element (maximum or minimum).
2. Replace the root with the last element in the heap.
3. "Bubble down" the new root element to restore the heap property by comparing it with its children and swapping it with the larger (or smaller) child until the heap property is satisfied.

Heap Sort:

Heap sort is a comparison-based sorting algorithm that uses a binary heap data structure to sort elements. 

It works by first building a max heap (or min heap) from the input data and then repeatedly extracting the maximum (or minimum) element from the heap to create a sorted array.

Heap Sort Steps:

1. Build a max heap (or min heap) from the input data.
2. Swap the root element (maximum or minimum) with the last element in the heap.
3. Reduce the size of the heap by removing the last element.
4. "Bubble down" the new root element to restore the heap property.
5. Repeat steps 2-4 until the heap is empty.

Heap Sort Complexity:
- Time Complexity: O(n log n) - Building the heap takes O(n) time, and each extraction takes O(log n) time.

- Space Complexity: O(1) - Heap sort is an in-place sorting algorithm, meaning it requires a constant amount of additional space.

Heap sort is an efficient sorting algorithm that can be used to sort large datasets. 

It is not a stable sort, meaning that the relative order of equal elements may not be preserved.

Priority Queue:

A priority queue is an abstract data type that operates similarly to a regular queue but with an added feature: each element has a priority associated with it. 

Elements with higher priority are dequeued before elements with lower priority, regardless of their order in the queue.

Priority queues can be implemented using heaps, where the highest (or lowest) priority element is always at the root of the heap. 

This allows for efficient insertion and deletion operations.